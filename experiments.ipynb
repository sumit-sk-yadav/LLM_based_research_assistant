{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'data\\\\DVSTUDY_PAPER.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c449e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30f0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af42ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_all_pdfs(folder_path: str) -> list:\n",
    "    pages = []\n",
    "    pdf_files = Path(folder_path).rglob(\"*.pdf\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc8badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = await load_all_pdfs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7450a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pages[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8f7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_splitter(pages:list[str], c_size: int, c_overlap: int) -> list:\n",
    "    chunks = []\n",
    "    if pages:\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size = c_size,chunk_overlap=c_overlap)\n",
    "            chunks = splitter.split_documents(pages)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter(pages, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121d233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a082e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"intfloat/e5-base-v2\"\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(chunks, embedding=HuggingFaceEmbeddings(model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d9efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "db2 = FAISS.from_documents(chunks, embedding=HuggingFaceEmbeddings(model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c1f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"thenlper/gte-base\"\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "db3 = FAISS.from_documents(chunks, embedding=HuggingFaceEmbeddings(model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52013856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb4e699b3d64a4fb8662c1c7052dad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   5%|4         | 21.0M/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ea535815c64a9ca4256d6744b7bedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c6abf1d4e64cbdb3c9114e9d436ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13148dc87d477c9a65a39950652a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d89cebd5d46416caed5d1a387453741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d70947208f49a0916548aac8093e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "db4 = FAISS.from_documents(chunks, embedding=HuggingFaceEmbeddings(model_name=model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# --- Embedding wrapper using thenlper/gte-base ---\n",
    "class GTEEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"thenlper/gte-base\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "\n",
    "# --- Utility: Hash documents for caching ---\n",
    "def compute_documents_hash(documents: List[Document]) -> str:\n",
    "    hasher = hashlib.sha256()\n",
    "    for doc in documents:\n",
    "        hasher.update(doc.page_content.encode(\"utf-8\"))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "# --- Main function: Embed and cache ---\n",
    "def embed_and_store_once(\n",
    "    documents: List[Document],\n",
    "    persist_dir: str = \"embeddings\",\n",
    "    model_name: str = \"thenlper/gte-base\"\n",
    ") -> Chroma:\n",
    "\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    hash_path = Path(persist_dir) / \"hash.pkl\"\n",
    "    current_hash = compute_documents_hash(documents)\n",
    "\n",
    "    # Check for previously stored hash\n",
    "    if hash_path.exists():\n",
    "        with open(hash_path, \"rb\") as f:\n",
    "            saved_hash = pickle.load(f)\n",
    "        if saved_hash == current_hash:\n",
    "            print(\"ðŸŸ¢ Reusing existing ChromaDB vector store from 'embeddings/'\")\n",
    "            return Chroma(\n",
    "                persist_directory=persist_dir,\n",
    "                embedding_function=GTEEmbeddings(model_name),\n",
    "                client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "            )\n",
    "\n",
    "    # Embed and store if hash differs\n",
    "    print(\"ðŸ”µ Generating new embeddings and storing in 'embeddings/'...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=GTEEmbeddings(model_name),\n",
    "        persist_directory=persist_dir,\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )\n",
    "\n",
    "    # Save hash for reuse\n",
    "    with open(hash_path, \"wb\") as f:\n",
    "        pickle.dump(current_hash, f)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00a28e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Reusing existing ChromaDB vector store from 'embddings/'\n"
     ]
    }
   ],
   "source": [
    "vectorstore = embed_and_store_once(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
