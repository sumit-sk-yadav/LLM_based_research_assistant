{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e21bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'data\\\\DVSTUDY.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c449e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30f0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af42ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_all_pdfs(folder_path: str) -> list:\n",
    "    pages = []\n",
    "    pdf_files = Path(folder_path).rglob(\"*.pdf\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcc8badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = await load_all_pdfs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7450a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pages[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f8f7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "separators = [\n",
    "    \"\\n\\n\",              # Paragraphs\n",
    "    \"\\n\",                # New lines\n",
    "    r\"\\.\\s\",             # Sentences\n",
    "    r\"(?:Fig\\.|Table)\\s\\d+\",  # Split around figures/tables\n",
    "    r\"\\s{2,}\",           # Double spaces (used in some PDFs)\n",
    "    \" \",                 # Words\n",
    "    \"\"                   # Fallback\n",
    "]\n",
    "\n",
    "def text_splitter(pages:list[str], c_size: int, c_overlap: int) -> list:\n",
    "    chunks = []\n",
    "    if pages:\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size = c_size, separators=separators,chunk_overlap=c_overlap)\n",
    "            chunks = splitter.split_documents(pages)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4d630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter(pages, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf978287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "cleaned_chunks = [Document(page_content=clean_text(doc.page_content), metadata=doc.metadata) for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "121d233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cleaned_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adaad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# --- Embedding wrapper using thenlper/gte-base ---\n",
    "class GTEEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"thenlper/gte-small\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "\n",
    "# --- Utility: Hash documents for caching ---\n",
    "def compute_documents_hash(documents: List[Document]) -> str:\n",
    "    hasher = hashlib.sha256()\n",
    "    for doc in documents:\n",
    "        hasher.update(doc.page_content.encode(\"utf-8\"))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "# --- Main function: Embed and cache ---\n",
    "def embed_and_store_once(\n",
    "    documents: List[Document],\n",
    "    persist_dir: str = \"embeddings\",\n",
    "    model_name: str = \"thenlper/gte-small\"\n",
    ") -> Chroma:\n",
    "\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    hash_path = Path(persist_dir) / \"hash.pkl\"\n",
    "    current_hash = compute_documents_hash(documents)\n",
    "\n",
    "    # Check for previously stored hash\n",
    "    if hash_path.exists():\n",
    "        with open(hash_path, \"rb\") as f:\n",
    "            saved_hash = pickle.load(f)\n",
    "        if saved_hash == current_hash:\n",
    "            print(\"ðŸŸ¢ Reusing existing ChromaDB vector store from 'embeddings/'\")\n",
    "            return Chroma(\n",
    "                persist_directory=persist_dir,\n",
    "                embedding_function=GTEEmbeddings(model_name),\n",
    "                client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "            )\n",
    "\n",
    "    # Embed and store if hash differs\n",
    "    print(\"ðŸ”µ Generating new embeddings and storing in 'embeddings/'...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=GTEEmbeddings(model_name),\n",
    "        persist_directory=persist_dir,\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )\n",
    "\n",
    "    # Save hash for reuse\n",
    "    with open(hash_path, \"wb\") as f:\n",
    "        pickle.dump(current_hash, f)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00a28e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Reusing existing ChromaDB vector store from 'embeddings/'\n"
     ]
    }
   ],
   "source": [
    "vectorstore = embed_and_store_once(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c62368d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Association index = O â€“ E Ïƒ where O is the observed co-occurrence of a species pair, E is the ex- pected co-occurrence of the pair and Ïƒ is the standard deviation of the expected co-occurrence of the species. The expected co-occurrence was calculated from randomizations on the species by flock presence absence matrix. Randomizations were set up in the following manner: Since we were interested in examining differences in flocks of different rich - ness values, we kept the number of flocks in each richness class in our expected data equal to the number of flocks in the observed data- set. The observed data matrix was randomized by holding the column totals (flock richness) constant and using the species occurrences as proportions. For each randomized matrix, we calculated a co-occur - rence value for every species pair. We performed 1000 iterations and'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Association index\"\n",
    "result = vectorstore.similarity_search(query, k=5)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c3fe9",
   "metadata": {},
   "source": [
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub  # Or any LLM model of your choice\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "PERSIST_DIR = os.path.join(\"embeddings\")  # Path where Chroma DB is persisted\n",
    "EMBEDDING_MODEL = \"thenlper/gte-small\"    # Pre-trained embedding model\n",
    "LLM_MODEL = \"google/flan-t5-base\"         # Or any other LLM model\n",
    "TOP_K = 5                                 # Number of top results to retrieve from vector store\n",
    "\n",
    "def load_vectorstore(persist_dir: str = PERSIST_DIR, model_name: str = EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Loads the vector store from the specified directory and embedding model.\n",
    "    \"\"\"\n",
    "    embedding_function = GTEEmbeddings(model_name=model_name)\n",
    "\n",
    "    logger.info(f\"Loading vector store from {persist_dir}...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding_function,\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(query: str, vectorstore: Chroma, top_k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    Retrieves the top K most relevant documents for the provided query from the vector store.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Retrieving top {top_k} most relevant documents for query: {query}\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "\n",
    "def setup_llm_model(llm_model: str = LLM_MODEL):\n",
    "    \"\"\"\n",
    "    Sets up the LLM model for question answering.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Setting up LLM model: {llm_model}...\")\n",
    "    llm = HuggingFaceHub(repo_id=llm_model, model_kwargs={\"temperature\": 0.2, \"max_length\": 512})\n",
    "    return llm\n",
    "\n",
    "\n",
    "def qa_chain_setup(llm, retriever):\n",
    "    \"\"\"\n",
    "    Set up the RetrievalQA chain which will use the retriever and LLM model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Setting up the RetrievalQA chain...\")\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "def retrieve_and_answer(query: str):\n",
    "    \"\"\"\n",
    "    Full pipeline to retrieve relevant documents and answer the question using the LLM.\n",
    "    \"\"\"\n",
    "    vectorstore = load_vectorstore()\n",
    "\n",
    "    documents = retrieve_relevant_documents(query, vectorstore)\n",
    "\n",
    "    if not documents:\n",
    "        logger.warning(\"No relevant documents found for the query.\")\n",
    "        return \"Sorry, I couldn't find any relevant information.\"\n",
    "\n",
    "    llm = setup_llm_model()\n",
    "    qa_chain = qa_chain_setup(llm, vectorstore.as_retriever())\n",
    "\n",
    "    result = qa_chain(query)\n",
    "\n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "\n",
    "    source_texts = [f\"Source {i+1}: {doc.page_content[:500]}...\" for i, doc in enumerate(sources)]\n",
    "    return answer, source_texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What does the association index mean in network analysis?\"\n",
    "    answer= retrieve_and_answer(query)\n",
    "    \n",
    "    print(\"\\nðŸ§  Answer:\", answer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "091af5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from typing import List\n",
    "\n",
    "class ChromaRetriever:\n",
    "    def __init__(self, vectorstore: Chroma, k: int = 5):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        return self.vectorstore.similarity_search(query, k=self.k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f2cbc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query: str, documents: List[Document]) -> str:\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    prompt = f\"\"\"You are an expert research assistant.\n",
    "\n",
    "Use the following context to answer the question as accurately as possible.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02e870ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def query_llama(prompt: str, model: str = \"llama3.2\") -> str:\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52b697b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_answer(query: str) -> str:\n",
    "    retriever = ChromaRetriever(vectorstore=vectorstore)\n",
    "    documents = retriever.retrieve(query)\n",
    "    prompt = build_prompt(query, documents)\n",
    "    answer = query_llama(prompt)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "68f2a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieve_and_answer(\"what is association index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e2ce7978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Association Index is a statistical measure used to quantify the level of association between two species in a flock. It is calculated as follows:\\n\\nAssociation Index = O â€“ E Ïƒ\\n\\nWhere:\\n- O is the observed co-occurrence of a species pair\\n- E is the expected co-occurrence of the pair, which was calculated from randomizations on the species by flock presence absence matrix\\n- Ïƒ is the standard deviation of the expected co-occurrence of the species.\\n\\nThis measure is used to filter out non-random associations in the network and retain only the significant or \"important\" interactions between species.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42854e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to vector_store script to retrieve the vector store\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.config import Settings\n",
    "\n",
    "from scripts.embedding import Embedder\n",
    "\n",
    "def load_vectorstore(\n",
    "    persist_dir: str = \"embeddings\",\n",
    "    model_name: str = \"thenlper/gte-small\"\n",
    ") -> Chroma:\n",
    "    return Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=Embedder(model_name),\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d0eefbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retriever\n",
    "\n",
    "class VectorstoreRetriever:\n",
    "    def __init__(self, vectorstore, k: int = 5):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.k = k\n",
    "\n",
    "    def retrieve(self, query: str):\n",
    "        return self.vectorstore.similarity_search(query, k=self.k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f482cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in script utils\n",
    "def build_prompt(query: str, documents) -> str:\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    return f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Use the following context to answer the user's question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "79275914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in rag chain.py file\n",
    "\n",
    "import ollama\n",
    "\n",
    "class RAGChain:\n",
    "    def __init__(self, vectorstore, model_name=\"llama3\", k=5):\n",
    "        self.retriever = VectorstoreRetriever(vectorstore, k)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def run(self, query: str) -> str:\n",
    "        documents = self.retriever.retrieve(query)\n",
    "        prompt = build_prompt(query, documents)\n",
    "        response = ollama.chat(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response[\"message\"][\"content\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
