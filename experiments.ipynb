{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'data\\\\DVSTUDY_PAPER.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c449e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30f0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af42ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def load_all_pdfs(folder_path: str) -> list:\n",
    "    pages = []\n",
    "    pdf_files = Path(folder_path).rglob(\"*.pdf\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(str(pdf_file))\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcc8badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = await load_all_pdfs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7450a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(pages[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8f7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "separators = [\n",
    "    \"\\n\\n\",              # Paragraphs\n",
    "    \"\\n\",                # New lines\n",
    "    r\"\\.\\s\",             # Sentences\n",
    "    r\"(?:Fig\\.|Table)\\s\\d+\",  # Split around figures/tables\n",
    "    r\"\\s{2,}\",           # Double spaces (used in some PDFs)\n",
    "    \" \",                 # Words\n",
    "    \"\"                   # Fallback\n",
    "]\n",
    "\n",
    "def text_splitter(pages:list[str], c_size: int, c_overlap: int) -> list:\n",
    "    chunks = []\n",
    "    if pages:\n",
    "        try:\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size = c_size, separators=separators,chunk_overlap=c_overlap)\n",
    "            chunks = splitter.split_documents(pages)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d630a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter(pages, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf978287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "cleaned_chunks = [Document(page_content=clean_text(doc.page_content), metadata=doc.metadata) for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121d233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(cleaned_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adaad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "# --- Embedding wrapper using thenlper/gte-base ---\n",
    "class GTEEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"thenlper/gte-small\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "\n",
    "# --- Utility: Hash documents for caching ---\n",
    "def compute_documents_hash(documents: List[Document]) -> str:\n",
    "    hasher = hashlib.sha256()\n",
    "    for doc in documents:\n",
    "        hasher.update(doc.page_content.encode(\"utf-8\"))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "\n",
    "# --- Main function: Embed and cache ---\n",
    "def embed_and_store_once(\n",
    "    documents: List[Document],\n",
    "    persist_dir: str = \"embeddings\",\n",
    "    model_name: str = \"thenlper/gte-small\"\n",
    ") -> Chroma:\n",
    "\n",
    "    os.makedirs(persist_dir, exist_ok=True)\n",
    "    hash_path = Path(persist_dir) / \"hash.pkl\"\n",
    "    current_hash = compute_documents_hash(documents)\n",
    "\n",
    "    # Check for previously stored hash\n",
    "    if hash_path.exists():\n",
    "        with open(hash_path, \"rb\") as f:\n",
    "            saved_hash = pickle.load(f)\n",
    "        if saved_hash == current_hash:\n",
    "            print(\"ðŸŸ¢ Reusing existing ChromaDB vector store from 'embeddings/'\")\n",
    "            return Chroma(\n",
    "                persist_directory=persist_dir,\n",
    "                embedding_function=GTEEmbeddings(model_name),\n",
    "                client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "            )\n",
    "\n",
    "    # Embed and store if hash differs\n",
    "    print(\"ðŸ”µ Generating new embeddings and storing in 'embeddings/'...\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=GTEEmbeddings(model_name),\n",
    "        persist_directory=persist_dir,\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )\n",
    "\n",
    "    # Save hash for reuse\n",
    "    with open(hash_path, \"wb\") as f:\n",
    "        pickle.dump(current_hash, f)\n",
    "\n",
    "    return vectorstore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00a28e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Reusing existing ChromaDB vector store from 'embeddings/'\n"
     ]
    }
   ],
   "source": [
    "vectorstore = embed_and_store_once(cleaned_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62368d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Association index = O â€“ E Ïƒ where O is the observed co-occurrence of a species pair, E is the ex- pected co-occurrence of the pair and Ïƒ is the standard deviation of the expected co-occurrence of the species. The expected co-occurrence was calculated from randomizations on the species by flock presence absence matrix. Randomizations were set up in the following manner: Since we were interested in examining differences in flocks of different rich - ness values, we kept the number of flocks in each richness class in our expected data equal to the number of flocks in the observed data- set. The observed data matrix was randomized by holding the column totals (flock richness) constant and using the species occurrences as proportions. For each randomized matrix, we calculated a co-occur - rence value for every species pair. We performed 1000 iterations and'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Association index\"\n",
    "result = vectorstore.similarity_search(query, k=5)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c3fe9",
   "metadata": {},
   "source": [
    "# scripts/retriever.py\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from chromadb.config import Settings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub  # Or any LLM model of your choice\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# === Configuration ===\n",
    "PERSIST_DIR = os.path.join(\"embeddings\")  # Path where Chroma DB is persisted\n",
    "EMBEDDING_MODEL = \"thenlper/gte-small\"    # Pre-trained embedding model\n",
    "LLM_MODEL = \"google/flan-t5-base\"         # Or any other LLM model\n",
    "TOP_K = 5                                 # Number of top results to retrieve from vector store\n",
    "\n",
    "def load_vectorstore(persist_dir: str = PERSIST_DIR, model_name: str = EMBEDDING_MODEL):\n",
    "    \"\"\"\n",
    "    Loads the vector store from the specified directory and embedding model.\n",
    "    \"\"\"\n",
    "    embedding_function = GTEEmbeddings(model_name=model_name)\n",
    "\n",
    "    logger.info(f\"Loading vector store from {persist_dir}...\")\n",
    "    vectorstore = Chroma(\n",
    "        persist_directory=persist_dir,\n",
    "        embedding_function=embedding_function,\n",
    "        client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False)\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(query: str, vectorstore: Chroma, top_k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    Retrieves the top K most relevant documents for the provided query from the vector store.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Retrieving top {top_k} most relevant documents for query: {query}\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    return retriever.get_relevant_documents(query)\n",
    "\n",
    "\n",
    "def setup_llm_model(llm_model: str = LLM_MODEL):\n",
    "    \"\"\"\n",
    "    Sets up the LLM model for question answering.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Setting up LLM model: {llm_model}...\")\n",
    "    llm = HuggingFaceHub(repo_id=llm_model, model_kwargs={\"temperature\": 0.2, \"max_length\": 512})\n",
    "    return llm\n",
    "\n",
    "\n",
    "def qa_chain_setup(llm, retriever):\n",
    "    \"\"\"\n",
    "    Set up the RetrievalQA chain which will use the retriever and LLM model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Setting up the RetrievalQA chain...\")\n",
    "    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "def retrieve_and_answer(query: str):\n",
    "    \"\"\"\n",
    "    Full pipeline to retrieve relevant documents and answer the question using the LLM.\n",
    "    \"\"\"\n",
    "    # Load the vector store\n",
    "    vectorstore = load_vectorstore()\n",
    "\n",
    "    # Retrieve relevant documents\n",
    "    documents = retrieve_relevant_documents(query, vectorstore)\n",
    "\n",
    "    if not documents:\n",
    "        logger.warning(\"No relevant documents found for the query.\")\n",
    "        return \"Sorry, I couldn't find any relevant information.\"\n",
    "\n",
    "    # Set up LLM model and QA chain\n",
    "    llm = setup_llm_model()\n",
    "    qa_chain = qa_chain_setup(llm, vectorstore.as_retriever())\n",
    "\n",
    "    # Use the chain to get the answer\n",
    "    result = qa_chain(query)\n",
    "\n",
    "    # Return the result along with source documents\n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "    \n",
    "    # Format source document output (optional)\n",
    "    source_texts = [f\"Source {i+1}: {doc.page_content[:500]}...\" for i, doc in enumerate(sources)]\n",
    "    return answer, source_texts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"What does the association index mean in network analysis?\"\n",
    "    answer= retrieve_and_answer(query)\n",
    "    \n",
    "    # Output answer and source context\n",
    "    print(\"\\nðŸ§  Answer:\", answer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cb3d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from scripts.embedding import Embedder\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def load_vectorstore(persist_dir: str = \"embeddings\", model_name:str = \"thenlper/gte-small\"):\n",
    "    embedding_fn = Embedder(model_name=model_name)\n",
    "    \n",
    "    vector_store = Chroma(persist_directory=persist_dir, embedding_function=embedding_fn,\n",
    "                            client_settings=Settings(persist_directory=persist_dir, anonymized_telemetry=False))\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(query:str, vector_store:Chroma, top_k:int = 5):\n",
    "    retriever = vector_store.as_retriever(search_kwargs = {\"k\": top_k})\n",
    "    \n",
    "    return retriever.get_relevant_documents(query=query)\n",
    "\n",
    "\n",
    "def setup_llm_model(llm_model:str = \"gemma3\"):\n",
    "    llm = OllamaLLM(model=llm_model, model_kwargs={\"temperature\": 0.2})\n",
    "\n",
    "    return llm\n",
    "\n",
    "\n",
    "def retriever_chain_setup(llm, retriever):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type=\"stuff\"  # Or another strategy like \"map_reduce\", if needed\n",
    "    )\n",
    "\n",
    "\n",
    "def retrieve_and_answer(query: str):\n",
    "    \n",
    "    vector_store = load_vectorstore()\n",
    "    \n",
    "    relevant_docs = retrieve_relevant_documents(query=query, vector_store=vector_store)\n",
    "    \n",
    "    llm = setup_llm_model(\"deepseek-r1\")\n",
    "    \n",
    "    qa_chain = retriever_chain_setup(llm, vector_store.as_retriever())\n",
    "    \n",
    "    result = qa_chain.invoke(query)\n",
    "    \n",
    "    answer = result['result']\n",
    "    sources = result['source_documents']\n",
    "    \n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b2df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_17764\\4133380709.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return retriever.get_relevant_documents(query=query)\n"
     ]
    }
   ],
   "source": [
    "answer, sources = retrieve_and_answer(\"what is modularity and how to calculate it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2769bc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6e0a2fe0-178c-4549-a1e2-29355960b179', metadata={'appligent': 'AppendPDF Pro 6.3 Linux 64 bit Aug 30 2019 Library 15.0.4', 'author': 'Priti Bangal, Hari Sridhar, Daizaburo Shizuka, Laura N. Vander Meiden, and Kartik Shankar', 'creationdate': '2021-12-13T12:20:46-08:00', 'creator': 'Appligent AppendPDF Pro 6.3', 'moddate': '2021-12-13T12:20:46-08:00', 'page': 9, 'page_label': '10', 'producer': 'Prince 12.5 (www.princexml.com)', 'source': 'data\\\\DVSTUDY_PAPER.pdf', 'title': 'Flock-species richness influences node importance and modularity in mixed-species flock networks', 'total_pages': 24}, page_content='We use unweighted networks for filtered associations. Hence, we use degree centrality as a measure of structural importance in this analysis. Therefore, there are multiple species with the same central- ity values in this category. Calculating modularity We ran a â€˜community detection algorithmâ€™ based on the Louvian method on the networks built using the meth - ods described above. Clusters of closely connected nodes (often termed â€˜communitiesâ€™ in network parlance) were detected based on modular- ity optimization (Newman 2006). Modularity is the measure of sepa- ration between two clusters calculated based on the number of edges within versus across clusters. In this method, each vertex is assigned to a unique cluster and a modularity score is calculated. At each step, the vertices are reassigned to clusters and a network structure that gives the maximum modularity is calculated. This process is repeated until modularity cannot be increased further.'),\n",
       " Document(id='48b2f88b-ce0e-430a-8dba-4966c09656b6', metadata={'appligent': 'AppendPDF Pro 6.3 Linux 64 bit Aug 30 2019 Library 15.0.4', 'author': 'Priti Bangal, Hari Sridhar, Daizaburo Shizuka, Laura N. Vander Meiden, and Kartik Shankar', 'creationdate': '2021-12-13T12:20:46-08:00', 'creator': 'Appligent AppendPDF Pro 6.3', 'moddate': '2021-12-13T12:20:46-08:00', 'page': 9, 'page_label': '10', 'producer': 'Prince 12.5 (www.princexml.com)', 'source': 'data\\\\DVSTUDY_PAPER.pdf', 'title': 'Flock-species richness influences node importance and modularity in mixed-species flock networks', 'total_pages': 24}, page_content='until modularity cannot be increased further. Comparing the structure of species associations across flock- richness using assortativity We measured how closely the modu - lar structure of larger networks reflected the patterns of associations in 2-species flocks. We reason that species associations in 2-species'),\n",
       " Document(id='cc1d1584-e39f-49f3-bda3-480b9956294b', metadata={'appligent': 'AppendPDF Pro 6.3 Linux 64 bit Aug 30 2019 Library 15.0.4', 'author': 'Priti Bangal, Hari Sridhar, Daizaburo Shizuka, Laura N. Vander Meiden, and Kartik Shankar', 'creationdate': '2021-12-13T12:20:46-08:00', 'creator': 'Appligent AppendPDF Pro 6.3', 'moddate': '2021-12-13T12:20:46-08:00', 'page': 9, 'page_label': '10', 'producer': 'Prince 12.5 (www.princexml.com)', 'source': 'data\\\\DVSTUDY_PAPER.pdf', 'title': 'Flock-species richness influences node importance and modularity in mixed-species flock networks', 'total_pages': 24}, page_content='Bangal et al. in Oecologia (2021) 9 whole species by flock matrix and each matrix was filtered into the corresponding flock-richness subsets to calculate the association in - dex for species pairs in every flock-richness category. Network measures Weighted degree We used weighted degree, which is a commonly used measure of centrality in networks as the measure of species structural importance in flocks. Weighted degree is a node-based mea- sure which is the sum of weights of all edges that pass through the node under consideration. n WD = âˆ‘ Wi i=1 Here, WD is the weighted degree of the node also referred to as node strength, W is the edge weight and i is the number of edges that pass through the node for which weighted degree is being calculated.'),\n",
       " Document(id='2310bfba-ec15-4c8f-af6f-b5529d2a41fb', metadata={'appligent': 'AppendPDF Pro 6.3 Linux 64 bit Aug 30 2019 Library 15.0.4', 'author': 'Priti Bangal, Hari Sridhar, Daizaburo Shizuka, Laura N. Vander Meiden, and Kartik Shankar', 'creationdate': '2021-12-13T12:20:46-08:00', 'creator': 'Appligent AppendPDF Pro 6.3', 'moddate': '2021-12-13T12:20:46-08:00', 'page': 0, 'page_label': '1', 'producer': 'Prince 12.5 (www.princexml.com)', 'source': 'data\\\\DVSTUDY_PAPER.pdf', 'title': 'Flock-species richness influences node importance and modularity in mixed-species flock networks', 'total_pages': 24}, page_content='Part of the Ecology and Evolutionary Biology Commons Bangal, Priti; Sridhar, Hari; Shizuka, Daizaburo; Vander Meiden, Laura N.; and Shankar, Kartik, \"Flock- species richness influences node importance and modularity in mixed-species flock networks\" (2021). Papers in Behavior and Biological Sciences. 85. https://digitalcommons.unl.edu/bioscibehavior/85 This Article is brought to you for free and open access by the Papers in the Biological Sciences at DigitalCommons@University of Nebraska - Lincoln. It has been accepted for inclusion in Papers in Behavior and Biological Sciences by an authorized administrator of DigitalCommons@University of Nebraska - Lincoln.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54a49b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to understand what modularity is and how it's calculated based on the provided context. Let me start by reading through the given information carefully.\\n\\nThe context talks about network analysis in ecology, specifically looking at mixed-species flocks. They're using a method called the Louvian algorithm for community detection, which aims to optimize modularity. Modularity measures the separation between clusters or communities in a network, with higher values indicating better-defined groups where there are more connections within groups than between them.\\n\\nThe context also mentions that modularity is calculated based on edges within versus across clusters. They start by assigning each vertex (node) to a unique cluster and then iteratively reassign nodes to maximize the modularity score until it can't be increased further.\\n\\nNow, I need to break this down step by step to explain how modularity works and how it's computed.\\n\\nFirst, what is a network in this context? It's a set of nodes (species) connected by edges (associations). Each edge has a weight representing the strength or frequency of association. The Louvian method uses these weights to group species into communities where associations are dense within groups and sparse between them.\\n\\nModularity quantifies how well the network can be divided into such communities. To calculate it, you compare the actual number of edges within communities (or modules) to the expected number if edges were distributed randomly. The formula for modularity typically involves summing over all pairs of nodes in each community, weighted by their connections, and subtracting a term based on the total weight and degree distribution.\\n\\nSo, step-by-step calculation would involve identifying the optimal partition of the network into communities using an algorithm like Louvain. Then, applying the modularity formula to measure how much better this partition is compared to random divisions.\\n\\nI should make sure I explain each part clearly: defining the network, community detection, and then calculating modularity based on intra-cluster vs inter-cluster edges.\\n</think>\\n\\nModularity in network analysis measures the extent to which a network can be divided into distinct groups (or communities) where connections are dense within groups and sparse between them. Here's how it's calculated:\\n\\n1. **Network Definition**: The network consists of nodes representing species and weighted edges indicating their associations.\\n\\n2. **Community Detection**: Using an algorithm like Louvain, the network is partitioned into communities to maximize modularity.\\n\\n3. **Modularity Calculation**:\\n   - For each community (module), sum the weights of all edges within that community.\\n   - Calculate the total weight in the network and determine the expected number of edges between nodes based on their degrees.\\n   - The modularity score is computed as the difference between actual intra-cluster edges and expected intra-cluster edges, normalized appropriately.\\n\\nIn summary, modularity quantifies how well a network can be divided into tightly-knit groups with sparse connections between them.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c7c14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_vector_search(query:str) -> str:\n",
    "    docs = retrieve_relevant_documents(query, vectorstore)\n",
    "    \n",
    "    return\"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5472d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "vector_store_tool = Tool(\n",
    "    name=\"LocalDocumentResearcher\",\n",
    "    func=local_vector_search,\n",
    "    description=\"Access local research PDF Files to find relevant information to the query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_17764\\2349309134.py:6: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "llm = setup_llm_model()\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools = [vector_store_tool],\n",
    "    llm = llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04905c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Explain the association index in network analysis.'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama import ChatOllama\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define the state schema\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "\n",
    "# Initialize the LLM\n",
    "llm2 = ChatOllama(model='llama3.2', temperature=0)\n",
    "\n",
    "# Define the tools used by the agent (ensure your vector_store_tool is correctly set up)\n",
    "tools = [vector_store_tool]\n",
    "\n",
    "# Create the agent node\n",
    "agent_node = create_react_agent(llm2, tools)\n",
    "\n",
    "# Initialize the state graph with the defined state schema\n",
    "graph = StateGraph(state_schema=AgentState)\n",
    "\n",
    "# Add the agent node to the graph\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "\n",
    "# Set the entry and finish points\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.set_finish_point(\"agent\")\n",
    "\n",
    "# Compile the graph into an executable agent\n",
    "executable_agent = graph.compile()\n",
    "\n",
    "# Invoke the agent with an input\n",
    "response = executable_agent.invoke({\"input\": \"Explain the association index in network analysis.\"})\n",
    "\n",
    "# Print the response to check output\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a50f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm2.invoke(\"what is an association index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8b6a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"An association index, also known as a co-occurrence matrix or a correlation matrix, is a mathematical table that shows the frequency of co-occurrences between two variables. It's a way to visualize and analyze the relationships between different variables in a dataset.\\n\\nIn general, an association index has the following characteristics:\\n\\n1. **Rows**: Represent one variable (e.g., a feature or attribute).\\n2. **Columns**: Represent another variable (e.g., another feature or attribute).\\n3. **Cells**: Contain the frequency of co-occurrences between the corresponding row and column variables.\\n\\nFor example, if we have two variables, X and Y, an association index might look like this:\\n\\n|  | Y=0 | Y=1 |\\n| --- | --- | --- |\\n| X=0 | 10 | 2 |\\n| X=1 | 5 | 8 |\\n\\nIn this example, the cell at row X=0 and column Y=0 contains a value of 10, indicating that there are 10 observations where both X and Y have values of 0. Similarly, the cell at row X=1 and column Y=1 contains a value of 8.\\n\\nAssociation indices can be used to:\\n\\n1. **Identify correlations**: By analyzing the association index, you can identify which variables tend to co-occur more frequently.\\n2. **Detect patterns**: The association index can reveal patterns in the data, such as clusters or outliers.\\n3. **Inform decision-making**: By understanding the relationships between variables, you can make more informed decisions about feature selection, model building, and other aspects of data analysis.\\n\\nThere are different types of association indices, including:\\n\\n1. **Pearson correlation coefficient**: Measures the linear relationship between two continuous variables.\\n2. **Mutual information**: Measures the mutual dependence between two discrete variables.\\n3. **Chi-squared test**: Tests for independence between two categorical variables.\\n\\nAssociation indices are widely used in various fields, such as statistics, machine learning, data mining, and data science.\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-26T07:35:28.4402205Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4962747100, 'load_duration': 17913400, 'prompt_eval_count': 30, 'prompt_eval_duration': 333303100, 'eval_count': 415, 'eval_duration': 4610472600, 'model_name': 'llama3.2'} id='run-42fac16d-2dac-4ace-8255-0921dc61c105-0' usage_metadata={'input_tokens': 30, 'output_tokens': 415, 'total_tokens': 445}\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39915800",
   "metadata": {},
   "outputs": [],
   "source": [
    "tooled_llm = ChatOllama(model='llama3.2', temperature=0.2).bind_tools([vector_store_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84fe75f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tooled_llm.invoke(\"find information about association index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "088f2455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'LocalDocumentResearcher', 'args': {'__arg1': 'association index'}, 'id': '0e4c6d5a-f6f4-4e40-bea2-56fbedf00b01', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d32ca808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "tools = [vector_store_tool]\n",
    "\n",
    "agent_node = create_react_agent(tooled_llm, tools)\n",
    "\n",
    "# --- Define the state schema for the graph ---\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: Annotated[str, None]  # Output will be filled later\n",
    "\n",
    "# --- Build the LangGraph ---\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.set_finish_point(\"agent\")\n",
    "\n",
    "# --- Compile the graph ---\n",
    "executable_agent = graph.compile()\n",
    "\n",
    "# --- Run the agent ---\n",
    "response = executable_agent.invoke({\"input\": \"Explain the association index in network analysis.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c9f8be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain the association index in network analysis.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdd537bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got Explain the association index in network analysis.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidUpdateError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m executable_agent = graph.compile()\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Run asynchronously\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m executable_agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mExplain the association index in network analysis.\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Show output\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2850\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2849\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2850\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2851\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2852\u001b[39m     config,\n\u001b[32m   2853\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2854\u001b[39m     output_keys=output_keys,\n\u001b[32m   2855\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2856\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2857\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2858\u001b[39m     debug=debug,\n\u001b[32m   2859\u001b[39m     **kwargs,\n\u001b[32m   2860\u001b[39m ):\n\u001b[32m   2861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2862\u001b[39m         latest = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2732\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2726\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2727\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2728\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2729\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2730\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2733\u001b[39m         loop.tasks.values(),\n\u001b[32m   2734\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2735\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2736\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2737\u001b[39m     ):\n\u001b[32m   2738\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2739\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2740\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\runner.py:283\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    281\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    284\u001b[39m         t,\n\u001b[32m    285\u001b[39m         retry_policy,\n\u001b[32m    286\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    287\u001b[39m         configurable={\n\u001b[32m    288\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    289\u001b[39m                 _acall,\n\u001b[32m    290\u001b[39m                 weakref.ref(t),\n\u001b[32m    291\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    292\u001b[39m                 retry=retry_policy,\n\u001b[32m    293\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    294\u001b[39m                 schedule_task=\u001b[38;5;28mself\u001b[39m.schedule_task,\n\u001b[32m    295\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    296\u001b[39m                 reraise=reraise,\n\u001b[32m    297\u001b[39m                 loop=loop,\n\u001b[32m    298\u001b[39m             ),\n\u001b[32m    299\u001b[39m         },\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\retry.py:128\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, configurable)\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\utils\\runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36magent_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magent_node\u001b[39m(state: AgentState) -> AgentState:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m base_agent.ainvoke(state[\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Debugging step\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[agent_node] Raw result from base_agent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2850\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2849\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2850\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2851\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2852\u001b[39m     config,\n\u001b[32m   2853\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2854\u001b[39m     output_keys=output_keys,\n\u001b[32m   2855\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2856\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2857\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2858\u001b[39m     debug=debug,\n\u001b[32m   2859\u001b[39m     **kwargs,\n\u001b[32m   2860\u001b[39m ):\n\u001b[32m   2861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2862\u001b[39m         latest = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2732\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2726\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2727\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2728\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2729\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2730\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2731\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2732\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2733\u001b[39m         loop.tasks.values(),\n\u001b[32m   2734\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2735\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2736\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2737\u001b[39m     ):\n\u001b[32m   2738\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2739\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2740\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\runner.py:283\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    281\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    284\u001b[39m         t,\n\u001b[32m    285\u001b[39m         retry_policy,\n\u001b[32m    286\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    287\u001b[39m         configurable={\n\u001b[32m    288\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    289\u001b[39m                 _acall,\n\u001b[32m    290\u001b[39m                 weakref.ref(t),\n\u001b[32m    291\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    292\u001b[39m                 retry=retry_policy,\n\u001b[32m    293\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    294\u001b[39m                 schedule_task=\u001b[38;5;28mself\u001b[39m.schedule_task,\n\u001b[32m    295\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    296\u001b[39m                 reraise=reraise,\n\u001b[32m    297\u001b[39m                 loop=loop,\n\u001b[32m    298\u001b[39m             ),\n\u001b[32m    299\u001b[39m         },\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    301\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\retry.py:128\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policies, stream, configurable)\u001b[39m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\utils\\runnable.py:440\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\write.py:114\u001b[39m, in \u001b[36mChannelWrite._awrite\u001b[39m\u001b[34m(self, input, config)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_awrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m     writes = [\n\u001b[32m    107\u001b[39m         ChannelWriteEntry(write.channel, \u001b[38;5;28minput\u001b[39m, write.skip_none, write.mapper)\n\u001b[32m    108\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write.value \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.writes\n\u001b[32m    113\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_write\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\write.py:142\u001b[39m, in \u001b[36mChannelWrite.do_write\u001b[39m\u001b[34m(config, writes, allow_passthrough, require_at_least_one_of)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# if we want to persist writes found before hitting a ParentCommand\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# can move this to a finally block\u001b[39;00m\n\u001b[32m    141\u001b[39m write: TYPE_SEND = config[CONF][CONFIG_KEY_SEND]\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m write(\u001b[43m_assemble_writes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\pregel\\write.py:199\u001b[39m, in \u001b[36m_assemble_writes\u001b[39m\u001b[34m(writes)\u001b[39m\n\u001b[32m    197\u001b[39m     tuples.append((TASKS, w))\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteTupleEntry):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ww := \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    200\u001b[39m         tuples.extend(ww)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(w, ChannelWriteEntry):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML Projects\\LLM_based_research_assistant\\researcher\\Lib\\site-packages\\langgraph\\graph\\state.py:767\u001b[39m, in \u001b[36mCompiledStateGraph.attach_node.<locals>._get_updates\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    763\u001b[39m     msg = create_error_message(\n\u001b[32m    764\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    765\u001b[39m         error_code=ErrorCode.INVALID_GRAPH_NODE_RETURN_VALUE,\n\u001b[32m    766\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(msg)\n",
      "\u001b[31mInvalidUpdateError\u001b[39m: Expected dict, got Explain the association index in network analysis.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
      "During task with name '__start__' and id 'e1064acd-678e-05e0-07dd-f9964c4d85ac'",
      "During task with name 'agent' and id '06ccd342-e65a-7748-c67f-1d55dd2b47f1'"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"llama3\", temperature=0)\n",
    "\n",
    "tools = [vector_store_tool]\n",
    "\n",
    "# Base agent\n",
    "base_agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Define your state\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    output: Annotated[str, None]\n",
    "\n",
    "# Wrap agent so it returns 'output'\n",
    "async def agent_node(state: AgentState) -> AgentState:\n",
    "    result = await base_agent.ainvoke(state[\"input\"])\n",
    "    \n",
    "    # Debugging step\n",
    "    print(f\"[agent_node] Raw result from base_agent: {result}\")\n",
    "\n",
    "    # Handle result\n",
    "    if hasattr(result, \"content\"):\n",
    "        output_content = result.content\n",
    "    else:\n",
    "        output_content = result  # Assume plain text if no .content\n",
    "\n",
    "    return {\n",
    "        \"input\": state[\"input\"],\n",
    "        \"output\": output_content\n",
    "    }\n",
    "# Build LangGraph\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.set_finish_point(\"agent\")\n",
    "executable_agent = graph.compile()\n",
    "\n",
    "# Run asynchronously\n",
    "response = await executable_agent.ainvoke({\"input\": \"Explain the association index in network analysis.\"})\n",
    "\n",
    "# Show output\n",
    "print(\"\\nFinal response:\")\n",
    "print(response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researcher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
